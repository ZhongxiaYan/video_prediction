{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Joint Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'labels/'\n",
    "labeled_frames = []\n",
    "for filename in os.listdir(directory):\n",
    "    annotations = loadmat(directory + filename)\n",
    "    if annotations['action'][0] == 'squat':\n",
    "        frame_height, frame_width = annotations['dimensions'][0][:2]\n",
    "        # Create Nx13x2 joint labels for each video\n",
    "        xy = np.stack([annotations['x'], annotations['y']], axis=2).astype(float)\n",
    "        bboxes = annotations['bbox']\n",
    "        x_min, y_min = np.min(bboxes[:, :2], axis=0)\n",
    "        x_max, y_max = np.max(bboxes[:, 2:], axis=0)\n",
    "        xy_min = np.array([x_min, y_min]).reshape(1, 1, 2)\n",
    "        xy_range = np.array([x_max - x_min, y_max - y_min]).reshape(1, 1, 2)\n",
    "        normed_coord = (xy - xy_min) / xy_range\n",
    "        normed_coord[normed_coord < 0] = 0\n",
    "        feature_dict = {\n",
    "            'file' : filename,\n",
    "            'nframes' : annotations['nframes'],\n",
    "            'pose' : annotations['pose'],\n",
    "            'coord' : xy,\n",
    "            'norm_coord' : normed_coord,\n",
    "            'visibility' : annotations['visibility'],\n",
    "            'y_min' : y_min,\n",
    "            'y_max' : y_max,\n",
    "            'x_min' : x_min,\n",
    "            'x_max' : x_max\n",
    "        }\n",
    "        labeled_frames.append(feature_dict)\n",
    "labeled_frames = sorted(labeled_frames, key=lambda x: x['file'])\n",
    "poses = set(f['pose'][0] for f in labeled_frames)\n",
    "print(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames = np.array([f['norm_coord'] for f in labeled_frames if f['pose'] == u'left' and f['nframes'] >= 70])\n",
    "indices = np.random.permutation(np.arange(len(selected_frames)))\n",
    "frames_train = selected_frames[indices[:65]]\n",
    "frames_test = selected_frames[indices[65:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 13 # num of joints\n",
    "# shortest video is length 25, so k + T = 25 in this case\n",
    "k = 15 # training num\n",
    "T = 45 # prediction num\n",
    "H = 1024 # hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(p, weights, biases):\n",
    "    # p should be shape (batch_size, T, 2 * L)\n",
    "    # unstack gets us a list of T (batch_size, 2 * L) tensors\n",
    "    batch_size = tf.shape(p)[0]\n",
    "    p = tf.unstack(p, k, axis=1)\n",
    "    lstm_cell = rnn.BasicLSTMCell(H, forget_bias=1.0)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, p, dtype=tf.float32)\n",
    "    \n",
    "    # Using generated output as input for next cell\n",
    "#     output_state = rnn.LSTMStateTuple(states[-1], outputs[-1])\n",
    "#     input_state = tf.matmul(outputs[-1], W) + b\n",
    "#     predictions = []\n",
    "#     for i in range(T):\n",
    "#         lstm_cell_pred = rnn.BasicLSTMCell(H, forget_bias=1.0, reuse=True)\n",
    "#         output, state = rnn.static_rnn(lstm_cell_pred, [input_state],\n",
    "#                                          initial_state=output_state,  dtype=tf.float32)\n",
    "        \n",
    "#         input_state = tf.matmul(output[0], W) + b\n",
    "        \n",
    "#         predictions.append(input_state)\n",
    "#         output_state = state\n",
    "    \n",
    "    output_state = rnn.LSTMStateTuple(states[-1], outputs[-1])\n",
    "    lstm_cell_pred = rnn.BasicLSTMCell(H, forget_bias=1.0, reuse=True)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell_pred, [tf.zeros((batch_size, L*2))] * T,\n",
    "                                     initial_state=output_state,  dtype=tf.float32)\n",
    "    \n",
    "    # outputs is a list of T (batch_size, H) arrays\n",
    "    # concat outputs is (batch_size * T, H)\n",
    "    concat_outputs = tf.concat(outputs, axis=0)\n",
    "    \n",
    "    # predictions is (batch_size * T, 2 * L)\n",
    "    predictions = tf.nn.sigmoid(tf.matmul(concat_outputs, W) + b)\n",
    "    predictions = tf.sigmoid(tf.matmul(predictions, W1) + b1)\n",
    "    \n",
    "    # reshape into (T, batch_size, 2 * L) then transpose into (batch_size, T, 2 * L)\n",
    "    return tf.transpose(tf.reshape(predictions, (T, batch_size, L * 2)), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "p_input = tf.placeholder(tf.float32, shape=[None, k, L*2])\n",
    "p_output = tf.placeholder(tf.float32, shape=[p_input.get_shape()[0], T, L*2])\n",
    "\n",
    "W = tf.get_variable('W', shape=[H, 100], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b', shape=[100], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "W1 = tf.get_variable('W1', shape=[100, L*2], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable('b1', shape=[L*2], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "p_output_predicted = RNN(p_input, W, b)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.squared_difference(p_output_predicted, p_output))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4000\n",
    "batch_size = 1\n",
    "n_videos = len(frames_train)\n",
    "display_step = 50\n",
    "save_step = 500\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mean_losses = []\n",
    "for epoch in range(epochs):\n",
    "    total_iter = n_videos // batch_size\n",
    "    total_loss = 0\n",
    "    for i in range(total_iter):\n",
    "        inputs = []\n",
    "        expected_outputs = []\n",
    "        for frame in frames_train:\n",
    "            start_time = np.random.randint(frame.shape[0] - (k + T))\n",
    "            inputs.append(frame[start_time : start_time + k].reshape(k, 2 * L))\n",
    "            expected_outputs.append(frame[start_time + k : start_time + k + T].reshape(T, 2 * L))\n",
    "        _, loss_value = sess.run((optimizer, loss), feed_dict={ p_input : np.asarray(inputs), p_output : np.asarray(expected_outputs) })\n",
    "        total_loss += loss_value\n",
    "    mean_loss = total_loss / total_iter\n",
    "    mean_losses.append(mean_loss)\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print('epoch %s: loss=%.8f' % (epoch + 1, mean_loss))\n",
    "    if (epoch + 1) % save_step == 0:\n",
    "        saver.save(sess, 'models/lstm-zee', global_step=(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'models/lstm-zee', global_step=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_ = [frame for frame in frames_test if frame.shape[0] >= k + T]\n",
    "inputs = []\n",
    "expected_outputs = []\n",
    "for frame in frames_:\n",
    "    start_time = np.random.randint(frame.shape[0] - (k + T) + 1)\n",
    "#     start_time = 0\n",
    "    inputs.append(frame[start_time : start_time + k].reshape(k, 2 * L))\n",
    "    expected_outputs.append(frame[start_time + k: start_time + k + T].reshape(T, 2 * L))\n",
    "\n",
    "output = sess.run((p_output_predicted), feed_dict={ p_input : np.asarray(inputs)}).reshape(len(frames_), T, L, 2)\n",
    "expected_output = np.asarray(expected_outputs).reshape(len(frames_), T, L, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(T):\n",
    "    print(np.mean(np.linalg.norm(output[:,i,:,:] - expected_output[:,i,:,:], axis=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib import cm\n",
    "\n",
    "# videos: a list of (T, L, 2) arrays\n",
    "frame_data = [frame for frame in labeled_frames if frame['pose'] == u'left'][:6]\n",
    "videos = [frame['norm_coord'] for frame in frame_data]\n",
    "i = 6\n",
    "videos = [output[i], expected_output[i]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(0, 1), ylim=(0, 1))\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue', 'cyan', 'magenta', 'violet', 'black']\n",
    "lines = [ax.plot([], [], 'o', color=colors[i])[0] for i in range(len(videos))]\n",
    "\n",
    "def init():\n",
    "    [line.set_data([], []) for line in lines]\n",
    "    return lines\n",
    "\n",
    "def animate(t):\n",
    "    [line.set_data(*video[t].T) for line, video in zip(lines, videos)]\n",
    "    return lines\n",
    "nframes = min(len(video) for video in videos)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=nframes, interval=nframes, blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
